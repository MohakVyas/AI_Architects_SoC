{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import time"
      ],
      "metadata": {
        "id": "55hGCavduHi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "S9_FofTfDvTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "s_wb5YUDuSg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY3037Jvtpdo"
      },
      "outputs": [],
      "source": [
        "path = '/content/data.txt'\n",
        "with open(path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokenizer.fit_on_texts([text])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "rSpx2EFFvBmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = []\n",
        "for sentence in text.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1, len(tokenized_sentence)):\n",
        "    input_seq.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "qTQBNETTvcN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(x) for x in input_seq])\n",
        "max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK2--ecrvpXZ",
        "outputId": "1ce6a6cd-c1e5-4e69-f706-ee68e1c19066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_seq = pad_sequences(input_seq, maxlen= max_length, padding='pre')"
      ],
      "metadata": {
        "id": "7OtmOmcywxRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_seq[:,:-1]\n",
        "y = padded_input_seq[:,-1]\n"
      ],
      "metadata": {
        "id": "gLu7eoLKx0su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7A4agTNyPLt",
        "outputId": "b948496e-6844-4ebd-f2f9-d50741713f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3666, 230)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dpRD30Jyw8D",
        "outputId": "de50a28a-cbc8-4673-a6bc-c8fbf1d42675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3666,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o64WRB11zFA7",
        "outputId": "623573e0-d3c6-4fd5-a8a2-99b3dda2c06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1098"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, num_classes=1099)"
      ],
      "metadata": {
        "id": "vkEfdazNyz86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRYSihylzJbg",
        "outputId": "4233be8a-3777-4306-883f-902543ef0d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3666, 1099)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM"
      ],
      "metadata": {
        "id": "AP6ZIZvfzbzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(1099, 100, input_length = 230))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(1099, activation='softmax'))"
      ],
      "metadata": {
        "id": "xbbq772Hz4Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jUWoSxmaz-r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vOlkTMx04uc",
        "outputId": "27dd5543-6092-4bf6-b7e8-947b809d2760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 230, 100)          109900    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1099)              165949    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 426449 (1.63 MB)\n",
            "Trainable params: 426449 (1.63 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXTm2YjY05qx",
        "outputId": "0294aced-f29a-4b08-b67d-a6bdf556b6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,    0,    2],\n",
              "       [   0,    0,    0, ...,    0,    2,   12],\n",
              "       [   0,    0,    0, ...,    2,   12,   83],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,   51, 1097, 1098],\n",
              "       [   0,    0,    0, ..., 1097, 1098,  238],\n",
              "       [   0,    0,    0, ..., 1098,  238,    3]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=250)"
      ],
      "metadata": {
        "id": "n9TUk45M1Ifw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = 'Do you love '\n",
        "\n",
        "token_test = tokenizer.texts_to_sequences([test])[0]\n",
        "padded_token_test = pad_sequences([token_test], maxlen=230, padding='pre')\n",
        "\n",
        "print(padded_token_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpR_uwe_3Iex",
        "outputId": "7bd6b0a7-5043-4cbb-aa85-60414f59efa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0 251  21]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(padded_token_test)\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxUSSeIe4MBT",
        "outputId": "f4802a91-fe12-4c4b-8898-d2e6638dc2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos = np.argmax(model.predict(padded_token_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcGp4LgA4YgA",
        "outputId": "1a7fed4e-1dd2-4824-a701-718826bb98aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, index in tokenizer.word_index.items():\n",
        "  if index == pos:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVZph1q64dvl",
        "outputId": "68bcc82a-86cc-4b0c-cd2b-1b18fea36e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ever\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = 'I found the couple at tea beneath their'"
      ],
      "metadata": {
        "id": "rct5QHam6F2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  token_text = tokenizer.texts_to_sequences([test])[0]\n",
        "  padded_token_test = pad_sequences([token_text], maxlen=230, padding='pre')\n",
        "  pos = np.argmax(model.predict(padded_token_test))\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      test = test + \" \" + word\n",
        "      print(test)\n",
        "      time.sleep(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC3xaeWj4nnK",
        "outputId": "c3ba54d1-0d28-4905-89b9-c76537378194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n",
            "I found the couple at tea beneath their palm\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "I found the couple at tea beneath their palm trees\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "I found the couple at tea beneath their palm trees and\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "I found the couple at tea beneath their palm trees and mrs\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "I found the couple at tea beneath their palm trees and mrs gisburn's\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "I found the couple at tea beneath their palm trees and mrs gisburn's welcome\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "I found the couple at tea beneath their palm trees and mrs gisburn's welcome was\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "I found the couple at tea beneath their palm trees and mrs gisburn's welcome was so\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "I found the couple at tea beneath their palm trees and mrs gisburn's welcome was so genial\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "I found the couple at tea beneath their palm trees and mrs gisburn's welcome was so genial that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('lstm_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3MPtjrm6BZ0",
        "outputId": "b6704d2e-9ab4-44d2-8349-f497a5b3f557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Majority of this code is GPT generated as I didnt know shit about such cool things like Gradio and tho this part is copied, I would definetly make sure to learn more about it and\" use it in some future project that i might get a chance to work on"
      ],
      "metadata": {
        "id": "vBwoX9efCQxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained LSTM model\n",
        "model = load_model('/content/lstm_model.h5')\n",
        "\n",
        "# Define a function for model prediction\n",
        "def predict(query, num_words):\n",
        "    # Preprocess the input query\n",
        "    test = query\n",
        "    # Generate the predicted sequence\n",
        "    for _ in range(int(num_words)):\n",
        "        token_text = tokenizer.texts_to_sequences([test])[0]\n",
        "        padded_token_test = pad_sequences([token_text], maxlen=230, padding='pre')\n",
        "        pos = np.argmax(model.predict(padded_token_test))\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "          if index == pos:\n",
        "            test = test + \" \" + word\n",
        "    return test\n",
        "\n",
        "\n",
        "\n",
        "        # Reshape the input sequence\n",
        "    #     input_seq_reshaped = input_seq.reshape(1, -1, 1)\n",
        "\n",
        "    #     # Predict the next word\n",
        "    #     predicted_word = model.predict(input_seq_reshaped)\n",
        "\n",
        "    #     # Append the predicted word to the input sequence\n",
        "    #     input_seq = np.append(input_seq, predicted_word)\n",
        "\n",
        "    # # Postprocess the output sequence\n",
        "    # predicted_sentence = postprocess_sequence(input_seq)\n",
        "\n",
        "\n",
        "# Define a function to preprocess the input query\n",
        "def preprocess_query(query):\n",
        "    # Tokenize and preprocess the query as required by your model\n",
        "    # Example: convert query to a numerical sequence\n",
        "    sequence = np.array([ord(char) for char in query])\n",
        "    return sequence\n",
        "\n",
        "# Define a function to postprocess the output sequence\n",
        "def postprocess_sequence(sequence):\n",
        "    # Convert the numerical sequence back to text\n",
        "    sentence = ''.join([chr(int(num)) for num in sequence])\n",
        "    return sentence\n",
        "\n",
        "# Define the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Enter input query here...\"),\n",
        "        gr.Slider(minimum=1, maximum=100, step=1, label=\"Number of words to predict\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"LSTM Text Generation\",\n",
        "    description=\"Enter a query and the number of words to predict. The model will return the complete sentence.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "UpTCLPhA_Zzl",
        "outputId": "a5493ccc-b0fe-461b-fc18-cebdcb4c4f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://acf7ac9e2d6a25034e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://acf7ac9e2d6a25034e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}